use hdp_primitives::datalake::output::TaskFormatted;
use hdp_provider::evm::AbstractProviderResult;
use serde::Serialize;

use super::types::InputProcessModule;

/*
    input.json file that will be passed to the processor, is generated by this struct.
*/

#[derive(Serialize)]
pub struct RunnerInput {
    /// Batched tasks root of all tasks.
    pub task_root: String,
    /// if every tasks are pre computable, this can be Some
    #[serde(skip_serializing_if = "Option::is_none")]
    pub result_root: Option<String>,
    /// Fetched proofs per each fetch point.
    pub proofs: AbstractProviderResult,
    /// tasks compatible with v2
    pub tasks: Vec<InputTask>,
}

#[derive(Serialize)]
pub enum InputTask {
    #[serde(rename = "datalake_compute")]
    DatalakeCompute(TaskFormatted),
    #[serde(rename = "module")]
    Module(InputProcessModule),
}

impl RunnerInput {
    pub fn new(proofs: AbstractProviderResult, task_root: String, tasks: Vec<InputTask>) -> Self {
        Self {
            task_root,
            result_root: None,
            tasks,
            proofs,
        }
    }
}
